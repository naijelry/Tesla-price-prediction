{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install dask[dataframe]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snVZlK_2pc-N",
        "outputId": "1389233f-eed5-4628-8498-abdf96e1b41d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.5.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2.2.2)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n",
            "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (17.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.21.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.16.0)\n",
            "Downloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dask-expr\n",
            "Successfully installed dask-expr-1.1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nN6d7l0noagp"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load the dataset (ensure that it's not too large for memory)\n",
        "file_path = '/content/tesla_stock_data_final_cleaneddata(noduplciates_nomissingvalues).csv'  # Adjust path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Optimize the data types of columns to reduce memory usage\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "df['year'] = df['timestamp'].dt.year.astype('int16')\n",
        "df['month'] = df['timestamp'].dt.month.astype('int8')\n",
        "df['day_of_week'] = df['timestamp'].dt.dayofweek.astype('int8')\n",
        "df['week_of_year'] = df['timestamp'].dt.isocalendar().week.astype('int8')\n",
        "\n",
        "# Use 'float32' or 'int32' where applicable to save memory\n",
        "df['close'] = df['close'].astype('float32')\n",
        "df['high'] = df['high'].astype('float32')\n",
        "df['low'] = df['low'].astype('float32')\n",
        "df['open'] = df['open'].astype('float32')\n",
        "\n",
        "# Feature Engineering: Keep it simple with lag features and rolling stats\n",
        "# 1. Lag Features (7, 14, and 30 days)\n",
        "df['high_lag_7'] = df['high'].shift(7)\n",
        "df['low_lag_7'] = df['low'].shift(7)\n",
        "df['close_lag_7'] = df['close'].shift(7)\n",
        "\n",
        "df['high_lag_14'] = df['high'].shift(14)\n",
        "df['low_lag_14'] = df['low'].shift(14)\n",
        "df['close_lag_14'] = df['close'].shift(14)\n",
        "\n",
        "df['high_lag_30'] = df['high'].shift(30)\n",
        "df['low_lag_30'] = df['low'].shift(30)\n",
        "df['close_lag_30'] = df['close'].shift(30)\n",
        "\n",
        "# 2. Rolling Features (30-day rolling mean and std)\n",
        "df['close_rolling_mean_30'] = df['close'].rolling(window=30).mean().astype('float32')\n",
        "df['high_rolling_mean_30'] = df['high'].rolling(window=30).mean().astype('float32')\n",
        "df['low_rolling_mean_30'] = df['low'].rolling(window=30).mean().astype('float32')\n",
        "\n",
        "df['close_rolling_std_30'] = df['close'].rolling(window=30).std().astype('float32')\n",
        "df['high_rolling_std_30'] = df['high'].rolling(window=30).std().astype('float32')\n",
        "df['low_rolling_std_30'] = df['low'].rolling(window=30).std().astype('float32')\n",
        "\n",
        "# 3. Price Differences\n",
        "df['price_diff'] = (df['high'] - df['low']).astype('float32')\n",
        "df['close_open_diff'] = (df['close'] - df['open']).astype('float32')\n",
        "\n",
        "# 4. Moving Averages (50 and 200 days)\n",
        "df['close_50ma'] = df['close'].rolling(window=50).mean().astype('float32')\n",
        "df['close_200ma'] = df['close'].rolling(window=200).mean().astype('float32')\n",
        "\n",
        "# Drop rows with NaN values (created by rolling and lagging)\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Features and Target Variables\n",
        "X = df[['year', 'month', 'day_of_week', 'week_of_year',\n",
        "        'close_rolling_mean_30', 'high_rolling_mean_30', 'low_rolling_mean_30',\n",
        "        'close_rolling_std_30', 'high_rolling_std_30', 'low_rolling_std_30',\n",
        "        'price_diff', 'close_open_diff', 'close_50ma', 'close_200ma',\n",
        "        'high_lag_7', 'low_lag_7', 'close_lag_7',\n",
        "        'high_lag_14', 'low_lag_14', 'close_lag_14',\n",
        "        'high_lag_30', 'low_lag_30', 'close_lag_30']]\n",
        "\n",
        "# Target Variables (high and low prices)\n",
        "y_high = df['high']\n",
        "y_low = df['low']\n"
      ],
      "metadata": {
        "id": "nCjCfv7TpQy1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "Q9wfDgQ-qB_M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Features and Target Variables\n",
        "X = df[['year', 'month', 'day_of_week', 'week_of_year',\n",
        "        'close_rolling_mean_30', 'high_rolling_mean_30', 'low_rolling_mean_30',\n",
        "        'close_rolling_std_30', 'high_rolling_std_30', 'low_rolling_std_30',\n",
        "        'price_diff', 'close_open_diff', 'close_50ma', 'close_200ma',\n",
        "        'high_lag_7', 'low_lag_7', 'close_lag_7',\n",
        "        'high_lag_14', 'low_lag_14', 'close_lag_14',\n",
        "        'high_lag_30', 'low_lag_30', 'close_lag_30']]\n",
        "\n",
        "# Target Variables (high and low prices)\n",
        "y_high = df['high']\n",
        "y_low = df['low']\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train_high, y_test_high, y_train_low, y_test_low = train_test_split(\n",
        "    X, y_high, y_low, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "# Initialize LightGBM models\n",
        "model_high = lgb.LGBMRegressor(random_state=42)\n",
        "model_low = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# Train the models\n",
        "model_high.fit(X_train, y_train_high)\n",
        "model_low.fit(X_train, y_train_low)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_high = model_high.predict(X_test)\n",
        "y_pred_low = model_low.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics: R², MSE, and MAE\n",
        "\n",
        "# High price model evaluation\n",
        "r2_high = r2_score(y_test_high, y_pred_high)\n",
        "mse_high = mean_squared_error(y_test_high, y_pred_high)\n",
        "mae_high = mean_absolute_error(y_test_high, y_pred_high)\n",
        "\n",
        "# Low price model evaluation\n",
        "r2_low = r2_score(y_test_low, y_pred_low)\n",
        "mse_low = mean_squared_error(y_test_low, y_pred_low)\n",
        "mae_low = mean_absolute_error(y_test_low, y_pred_low)\n",
        "\n",
        "# Print the evaluation metrics for both models\n",
        "print(f\"High Price Model Evaluation Metrics:\")\n",
        "print(f\"R²: {r2_high:.4f}\")\n",
        "print(f\"MSE: {mse_high:.4f}\")\n",
        "print(f\"MAE: {mae_high:.4f}\")\n",
        "\n",
        "print(f\"\\nLow Price Model Evaluation Metrics:\")\n",
        "print(f\"R²: {r2_low:.4f}\")\n",
        "print(f\"MSE: {mse_low:.4f}\")\n",
        "print(f\"MAE: {mae_low:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if05XuxQpUrM",
        "outputId": "02ac6257-51a9-4fb3-caa3-bbab1189796a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171183 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4925\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193406, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 458.317211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169710 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4925\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193406, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 457.637015\n",
            "High Price Model Evaluation Metrics:\n",
            "R²: 0.9990\n",
            "MSE: 16.9495\n",
            "MAE: 0.9197\n",
            "\n",
            "Low Price Model Evaluation Metrics:\n",
            "R²: 0.9974\n",
            "MSE: 45.5177\n",
            "MAE: 4.0472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Assuming you have already trained your LightGBM models, for example:\n",
        "# model_high and model_low are your trained LightGBM models\n",
        "\n",
        "# Save the LightGBM models using joblib\n",
        "joblib.dump(model_high, '/content/best_lgbm_high_model.pkl')\n",
        "joblib.dump(model_low, '/content/best_lgbm_low_model.pkl')\n",
        "\n",
        "print(\"Models saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyXQH8xus350",
        "outputId": "9a4986b8-2dd4-4a2a-9d40-d06fca791576"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is already loaded as you did earlier\n",
        "\n",
        "def prepare_features_for_date(input_date, df):\n",
        "    \"\"\"\n",
        "    This function prepares the features for a given input date based on your\n",
        "    feature engineering approach.\n",
        "    \"\"\"\n",
        "    # Convert input date to datetime and localize to UTC if necessary\n",
        "    input_date = pd.to_datetime(input_date)\n",
        "\n",
        "    # Ensure the timestamp column is timezone-aware in UTC\n",
        "    if df['timestamp'].dt.tz is None:\n",
        "        df['timestamp'] = df['timestamp'].dt.tz_localize('UTC')\n",
        "\n",
        "    # If input_date is naive (without timezone), convert it to UTC as well\n",
        "    if input_date.tz is None:\n",
        "        input_date = input_date.tz_localize('UTC')\n",
        "\n",
        "    # Filter data for rows until the input date\n",
        "    df_filtered = df[df['timestamp'] <= input_date]\n",
        "    df_filtered = df_filtered.sort_values(by='timestamp')\n",
        "\n",
        "    # Get the last 30 rows for rolling features (or you can choose a different window)\n",
        "    historical_data = df_filtered.tail(30)\n",
        "\n",
        "    # Compute lag features and rolling statistics for the input date\n",
        "    features = {}\n",
        "\n",
        "    # Extract date-based features\n",
        "    features['year'] = input_date.year\n",
        "    features['month'] = input_date.month\n",
        "    features['day_of_week'] = input_date.weekday()\n",
        "    features['week_of_year'] = input_date.isocalendar().week\n",
        "\n",
        "    # Get rolling statistics (30-day window)\n",
        "    features['close_rolling_mean_30'] = historical_data['close'].mean()\n",
        "    features['high_rolling_mean_30'] = historical_data['high'].mean()\n",
        "    features['low_rolling_mean_30'] = historical_data['low'].mean()\n",
        "\n",
        "    features['close_rolling_std_30'] = historical_data['close'].std()\n",
        "    features['high_rolling_std_30'] = historical_data['high'].std()\n",
        "    features['low_rolling_std_30'] = historical_data['low'].std()\n",
        "\n",
        "    # Compute price differences\n",
        "    features['price_diff'] = historical_data['high'].iloc[-1] - historical_data['low'].iloc[-1]\n",
        "    features['close_open_diff'] = historical_data['close'].iloc[-1] - historical_data['open'].iloc[-1]\n",
        "\n",
        "    # Moving averages (50-day and 200-day) for close prices\n",
        "    features['close_50ma'] = historical_data['close'].tail(50).mean() if len(historical_data) >= 50 else np.nan\n",
        "    features['close_200ma'] = historical_data['close'].tail(200).mean() if len(historical_data) >= 200 else np.nan\n",
        "\n",
        "    # Lag features (e.g., for 7, 14, 30 days)\n",
        "    features['high_lag_7'] = historical_data['high'].iloc[-7] if len(historical_data) >= 7 else np.nan\n",
        "    features['low_lag_7'] = historical_data['low'].iloc[-7] if len(historical_data) >= 7 else np.nan\n",
        "    features['close_lag_7'] = historical_data['close'].iloc[-7] if len(historical_data) >= 7 else np.nan\n",
        "\n",
        "    features['high_lag_14'] = historical_data['high'].iloc[-14] if len(historical_data) >= 14 else np.nan\n",
        "    features['low_lag_14'] = historical_data['low'].iloc[-14] if len(historical_data) >= 14 else np.nan\n",
        "    features['close_lag_14'] = historical_data['close'].iloc[-14] if len(historical_data) >= 14 else np.nan\n",
        "\n",
        "    features['high_lag_30'] = historical_data['high'].iloc[-30] if len(historical_data) >= 30 else np.nan\n",
        "    features['low_lag_30'] = historical_data['low'].iloc[-30] if len(historical_data) >= 30 else np.nan\n",
        "    features['close_lag_30'] = historical_data['close'].iloc[-30] if len(historical_data) >= 30 else np.nan\n",
        "\n",
        "    return features\n",
        "\n",
        "# Predict the high and low prices for the input date\n",
        "def predict_for_date(input_date, df, model_high, model_low):\n",
        "    # Prepare the features for the given input date\n",
        "    features = prepare_features_for_date(input_date, df)\n",
        "\n",
        "    # Convert features into DataFrame for prediction\n",
        "    input_features = pd.DataFrame([features])\n",
        "\n",
        "    # Make predictions for high and low prices\n",
        "    predicted_high = model_high.predict(input_features)\n",
        "    predicted_low = model_low.predict(input_features)\n",
        "\n",
        "    # Adjust predictions if low price is greater than high price\n",
        "    if predicted_low > predicted_high:\n",
        "        # Swap the predictions if low price is greater than high price\n",
        "        predicted_high, predicted_low = predicted_low, predicted_high\n",
        "\n",
        "    # Return the predictions\n",
        "    return predicted_high[0], predicted_low[0]\n",
        "\n",
        "# Example usage\n",
        "input_date = '2025-01-22'  # Replace with the date you want to predict for\n",
        "df = pd.read_csv('/content/tesla_stock_data_final_cleaneddata(noduplciates_nomissingvalues).csv', parse_dates=['timestamp'])\n",
        "\n",
        "predicted_high, predicted_low = predict_for_date(input_date, df, model_high, model_low)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Predicted High Price for {input_date}: {predicted_high:.4f}\")\n",
        "print(f\"Predicted Low Price for {input_date}: {predicted_low:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndnY6xRjtUyk",
        "outputId": "963979b7-8a14-4fad-e9bf-7e4e3f384382"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted High Price for 2025-01-22: 240.1710\n",
            "Predicted Low Price for 2025-01-22: 239.7123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print features for debugging\n",
        "input_date = '2025-01-22'  # Use any date for testing\n",
        "features = prepare_features_for_date(input_date, df)\n",
        "print(\"Features for input date:\", features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COKnSQuKt-qE",
        "outputId": "11e74cc7-f448-40f5-8d55-008602bad940"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features for input date: {'year': 2025, 'month': 1, 'day_of_week': 2, 'week_of_year': 4, 'close_rolling_mean_30': 239.71108666666663, 'high_rolling_mean_30': 239.73929333333328, 'low_rolling_mean_30': 239.68667000000002, 'close_rolling_std_30': 0.060427116163324364, 'high_rolling_std_30': 0.047124925195831974, 'low_rolling_std_30': 0.06470932965830609, 'price_diff': 0.09999999999999432, 'close_open_diff': 0.09999999999999432, 'close_50ma': nan, 'close_200ma': nan, 'high_lag_7': 239.77, 'low_lag_7': 239.75, 'close_lag_7': 239.75, 'high_lag_14': 239.61, 'low_lag_14': 239.58, 'close_lag_14': 239.6, 'high_lag_30': 239.84, 'low_lag_30': 239.8, 'close_lag_30': 239.81}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with different dates\n",
        "dates_to_test = ['2025-01-22', '2025-02-01', '2025-03-01']\n",
        "for date in dates_to_test:\n",
        "    predicted_high, predicted_low = predict_for_date(date, df, model_high, model_low)\n",
        "    print(f\"Predicted High for {date}: {predicted_high:.4f}\")\n",
        "    print(f\"Predicted Low for {date}: {predicted_low:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_m1LGDpuCnk",
        "outputId": "293f96c6-25aa-4bf4-e279-97a3c1f4d1e1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted High for 2025-01-22: 240.1710\n",
            "Predicted Low for 2025-01-22: 239.7123\n",
            "Predicted High for 2025-02-01: 240.1710\n",
            "Predicted Low for 2025-02-01: 239.7123\n",
            "Predicted High for 2025-03-01: 240.1710\n",
            "Predicted Low for 2025-03-01: 239.7123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model evaluation metrics (MSE, MAE, etc.)\n",
        "y_pred_high_train = model_high.predict(X_train)\n",
        "y_pred_low_train = model_low.predict(X_train)\n",
        "\n",
        "# High price evaluation\n",
        "print(f\"Training High Price MAE: {mean_absolute_error(y_train_high, y_pred_high_train)}\")\n",
        "print(f\"Training High Price MSE: {mean_squared_error(y_train_high, y_pred_high_train)}\")\n",
        "\n",
        "# Low price evaluation\n",
        "print(f\"Training Low Price MAE: {mean_absolute_error(y_train_low, y_pred_low_train)}\")\n",
        "print(f\"Training Low Price MSE: {mean_squared_error(y_train_low, y_pred_low_train)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jq4vW6zuKNk",
        "outputId": "ed7a67a4-6bdd-4a26-fc0f-b9e03bf50f90"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training High Price MAE: 1.742615892138584\n",
            "Training High Price MSE: 18.767362042956403\n",
            "Training Low Price MAE: 1.7522237167479906\n",
            "Training Low Price MSE: 19.494061697537926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using cross-validation for LightGBM\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = cross_val_score(model_high, X, y_high, cv=5, scoring='neg_mean_squared_error')\n",
        "print(f\"High price model cross-validation scores (MSE): {-cv_scores}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M82wy-rtuee1",
        "outputId": "1c8d643f-5deb-40aa-8d18-98552e6ebec9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125381 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4928\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193406, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 481.039790\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113953 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4928\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193406, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 360.921766\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112652 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4928\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193406, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 436.126036\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172346 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4928\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193407, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 393.427407\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117928 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4925\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193407, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 458.317377\n",
            "High price model cross-validation scores (MSE): [1.49967317e+01 1.46782586e+04 3.03768318e+01 2.57570674e+04\n",
            " 1.49609884e+01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It seems like the cross-validation results for your high-price model are highly variable, with some values being very large (e.g., 1.46782586e+04 and 2.57570674e+04) and others being smaller (e.g., 1.49967317e+01 and 1.49609884e+01). This kind of inconsistency suggests that the model's performance is not stable across different subsets of your data, which could indicate that the model is either overfitting to some subsets or underfitting to others."
      ],
      "metadata": {
        "id": "_Oap3QbBvjEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "\n",
        "# Define the parameter grid for randomized search\n",
        "param_dist = {\n",
        "    'num_leaves': np.arange(20, 101, 10),  # Try different values for the number of leaves\n",
        "    'max_depth': [-1, 5, 10],               # Max depth of the trees\n",
        "    'learning_rate': [0.01, 0.05, 0.1],     # Learning rate\n",
        "    'n_estimators': [100, 200, 500],        # Number of trees in the model\n",
        "    'min_child_samples': [20, 50],          # Minimum number of samples required to form a leaf\n",
        "    'subsample': [0.7, 0.8, 0.9],           # Fraction of samples used for fitting each tree\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9]     # Fraction of features used for fitting each tree\n",
        "}\n",
        "\n",
        "# Initialize the model (this model will be used for RandomizedSearchCV)\n",
        "model_high = lgb.LGBMRegressor(random_state=42)\n",
        "model_low = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# Setup RandomizedSearchCV for hyperparameter tuning\n",
        "random_search_high = RandomizedSearchCV(estimator=model_high, param_distributions=param_dist,\n",
        "                                         n_iter=10, cv=3, n_jobs=-1,\n",
        "                                         scoring='neg_mean_squared_error', verbose=1, random_state=42)\n",
        "random_search_low = RandomizedSearchCV(estimator=model_low, param_distributions=param_dist,\n",
        "                                        n_iter=10, cv=3, n_jobs=-1,\n",
        "                                        scoring='neg_mean_squared_error', verbose=1, random_state=42)\n",
        "\n",
        "# Fit RandomizedSearchCV to find the best hyperparameters for the high price model\n",
        "random_search_high.fit(X_train, y_train_high)\n",
        "random_search_low.fit(X_train, y_train_low)\n",
        "\n",
        "# Get the best parameters and best score for both models\n",
        "print(f\"Best Parameters for High Price Model: {random_search_high.best_params_}\")\n",
        "print(f\"Best Score for High Price Model: {-random_search_high.best_score_}\")\n",
        "\n",
        "print(f\"Best Parameters for Low Price Model: {random_search_low.best_params_}\")\n",
        "print(f\"Best Score for Low Price Model: {-random_search_low.best_score_}\")\n",
        "\n",
        "# Retrieve the best models from randomized search\n",
        "best_model_high = random_search_high.best_estimator_\n",
        "best_model_low = random_search_low.best_estimator_\n",
        "\n",
        "# Train the best models on the entire training set\n",
        "best_model_high.fit(X_train, y_train_high)\n",
        "best_model_low.fit(X_train, y_train_low)\n",
        "\n",
        "# Make predictions with the best models\n",
        "y_pred_high = best_model_high.predict(X_test)\n",
        "y_pred_low = best_model_low.predict(X_test)\n",
        "\n",
        "# Evaluate the models using R², MSE, and MAE\n",
        "r2_high = r2_score(y_test_high, y_pred_high)\n",
        "mse_high = mean_squared_error(y_test_high, y_pred_high)\n",
        "mae_high = mean_absolute_error(y_test_high, y_pred_high)\n",
        "\n",
        "r2_low = r2_score(y_test_low, y_pred_low)\n",
        "mse_low = mean_squared_error(y_test_low, y_pred_low)\n",
        "mae_low = mean_absolute_error(y_test_low, y_pred_low)\n",
        "\n",
        "# Print the evaluation metrics for both models\n",
        "print(f\"High Price Model Evaluation Metrics:\")\n",
        "print(f\"R²: {r2_high:.4f}\")\n",
        "print(f\"MSE: {mse_high:.4f}\")\n",
        "print(f\"MAE: {mae_high:.4f}\")\n",
        "\n",
        "print(f\"\\nLow Price Model Evaluation Metrics:\")\n",
        "print(f\"R²: {r2_low:.4f}\")\n",
        "print(f\"MSE: {mse_low:.4f}\")\n",
        "print(f\"MAE: {mae_low:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msYcZRXmy-_t",
        "outputId": "d5b5a472-8f9b-4aae-c935-beec6e02d2f6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093439 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4925\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193406, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 457.637015\n",
            "Best Parameters for High Price Model: {'subsample': 0.7, 'num_leaves': 70, 'n_estimators': 200, 'min_child_samples': 20, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.9}\n",
            "Best Score for High Price Model: 6709.415584653631\n",
            "Best Parameters for Low Price Model: {'subsample': 0.9, 'num_leaves': 90, 'n_estimators': 200, 'min_child_samples': 20, 'max_depth': 10, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n",
            "Best Score for Low Price Model: 6827.374731158015\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108747 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4925\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193406, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 458.317211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090222 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4925\n",
            "[LightGBM] [Info] Number of data points in the train set: 1193406, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 457.637015\n",
            "High Price Model Evaluation Metrics:\n",
            "R²: 0.9984\n",
            "MSE: 28.2123\n",
            "MAE: 2.6480\n",
            "\n",
            "Low Price Model Evaluation Metrics:\n",
            "R²: 0.9992\n",
            "MSE: 13.2960\n",
            "MAE: 0.7875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the best models for both high and low price predictions\n",
        "joblib.dump(best_model_high, '/content/best_model_high_Tuned.pkl')\n",
        "joblib.dump(best_model_low, '/content/best_model_low_Tuned.pkl')\n",
        "\n",
        "print(\"Models have been saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1vRQn-p6IMW",
        "outputId": "33f0abf3-5922-4ab0-fbd7-a249432212ff"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models have been saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load the saved models\n",
        "model_high = joblib.load('/content/best_model_high_Tuned.pkl')  # Adjust path if needed\n",
        "model_low = joblib.load('/content/best_model_low_Tuned.pkl')    # Adjust path if needed\n",
        "\n",
        "# Prepare the features for the given input date\n",
        "def prepare_features_for_date(input_date, df):\n",
        "    \"\"\"\n",
        "    This function prepares the features for a given input date based on your\n",
        "    feature engineering approach.\n",
        "    \"\"\"\n",
        "    # Convert input date to datetime and localize to UTC if necessary\n",
        "    input_date = pd.to_datetime(input_date)\n",
        "\n",
        "    # Ensure the timestamp column is timezone-aware in UTC\n",
        "    if df['timestamp'].dt.tz is None:\n",
        "        df['timestamp'] = df['timestamp'].dt.tz_localize('UTC')\n",
        "\n",
        "    # If input_date is naive (without timezone), convert it to UTC as well\n",
        "    if input_date.tz is None:\n",
        "        input_date = input_date.tz_localize('UTC')\n",
        "\n",
        "    # Filter data for rows until the input date\n",
        "    df_filtered = df[df['timestamp'] <= input_date]\n",
        "    df_filtered = df_filtered.sort_values(by='timestamp')\n",
        "\n",
        "    # Get the last 30 rows for rolling features (or you can choose a different window)\n",
        "    historical_data = df_filtered.tail(30)\n",
        "\n",
        "    # Compute lag features and rolling statistics for the input date\n",
        "    features = {}\n",
        "\n",
        "    # Extract date-based features\n",
        "    features['year'] = input_date.year\n",
        "    features['month'] = input_date.month\n",
        "    features['day_of_week'] = input_date.weekday()\n",
        "    features['week_of_year'] = input_date.isocalendar().week\n",
        "\n",
        "    # Get rolling statistics (30-day window)\n",
        "    features['close_rolling_mean_30'] = historical_data['close'].mean()\n",
        "    features['high_rolling_mean_30'] = historical_data['high'].mean()\n",
        "    features['low_rolling_mean_30'] = historical_data['low'].mean()\n",
        "\n",
        "    features['close_rolling_std_30'] = historical_data['close'].std()\n",
        "    features['high_rolling_std_30'] = historical_data['high'].std()\n",
        "    features['low_rolling_std_30'] = historical_data['low'].std()\n",
        "\n",
        "    # Compute price differences\n",
        "    features['price_diff'] = historical_data['high'].iloc[-1] - historical_data['low'].iloc[-1]\n",
        "    features['close_open_diff'] = historical_data['close'].iloc[-1] - historical_data['open'].iloc[-1]\n",
        "\n",
        "    # Moving averages (50-day and 200-day) for close prices\n",
        "    features['close_50ma'] = historical_data['close'].tail(50).mean() if len(historical_data) >= 50 else np.nan\n",
        "    features['close_200ma'] = historical_data['close'].tail(200).mean() if len(historical_data) >= 200 else np.nan\n",
        "\n",
        "    # Lag features (e.g., for 7, 14, 30 days)\n",
        "    features['high_lag_7'] = historical_data['high'].iloc[-7] if len(historical_data) >= 7 else np.nan\n",
        "    features['low_lag_7'] = historical_data['low'].iloc[-7] if len(historical_data) >= 7 else np.nan\n",
        "    features['close_lag_7'] = historical_data['close'].iloc[-7] if len(historical_data) >= 7 else np.nan\n",
        "\n",
        "    features['high_lag_14'] = historical_data['high'].iloc[-14] if len(historical_data) >= 14 else np.nan\n",
        "    features['low_lag_14'] = historical_data['low'].iloc[-14] if len(historical_data) >= 14 else np.nan\n",
        "    features['close_lag_14'] = historical_data['close'].iloc[-14] if len(historical_data) >= 14 else np.nan\n",
        "\n",
        "    features['high_lag_30'] = historical_data['high'].iloc[-30] if len(historical_data) >= 30 else np.nan\n",
        "    features['low_lag_30'] = historical_data['low'].iloc[-30] if len(historical_data) >= 30 else np.nan\n",
        "    features['close_lag_30'] = historical_data['close'].iloc[-30] if len(historical_data) >= 30 else np.nan\n",
        "\n",
        "    return features\n",
        "\n",
        "# Predict the high and low prices for the input date\n",
        "def predict_for_date(input_date, df, model_high, model_low):\n",
        "    # Prepare the features for the given input date\n",
        "    features = prepare_features_for_date(input_date, df)\n",
        "\n",
        "    # Convert features into DataFrame for prediction\n",
        "    input_features = pd.DataFrame([features])\n",
        "\n",
        "    # Make predictions for high and low prices\n",
        "    predicted_high = model_high.predict(input_features)\n",
        "    predicted_low = model_low.predict(input_features)\n",
        "\n",
        "    # Adjust predictions if low price is greater than high price\n",
        "    if predicted_low > predicted_high:\n",
        "        # Swap the predictions if low price is greater than high price\n",
        "        predicted_high, predicted_low = predicted_low, predicted_high\n",
        "\n",
        "    # Return the predictions\n",
        "    return predicted_high[0], predicted_low[0]\n",
        "\n",
        "# Example usage\n",
        "input_date = '2026-11-22'  # Replace with the date you want to predict for\n",
        "df = pd.read_csv('/content/tesla_stock_data_final_cleaneddata(noduplciates_nomissingvalues).csv', parse_dates=['timestamp'])\n",
        "\n",
        "predicted_high, predicted_low = predict_for_date(input_date, df, model_high, model_low)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Predicted High Price for {input_date}: {predicted_high:.4f}\")\n",
        "print(f\"Predicted Low Price for {input_date}: {predicted_low:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNZzlxS86r1H",
        "outputId": "973943cd-1160-4f10-b0b4-948483ce5c20"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted High Price for 2026-11-22: 240.0223\n",
            "Predicted Low Price for 2026-11-22: 238.1804\n"
          ]
        }
      ]
    }
  ]
}